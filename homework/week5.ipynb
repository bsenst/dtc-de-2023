{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2023/week_5_batch_processing/homework.md\n\n! pip install pyspark","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-24T21:31:29.501317Z","iopub.execute_input":"2023-02-24T21:31:29.501870Z","iopub.status.idle":"2023-02-24T21:31:40.192565Z","shell.execute_reply.started":"2023-02-24T21:31:29.501837Z","shell.execute_reply":"2023-02-24T21:31:40.190985Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyspark in /opt/conda/lib/python3.7/site-packages (3.3.2)\nRequirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.7/site-packages (from pyspark) (0.10.9.5)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"dtc-de-2023-nyc-taxi\").getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2023-02-24T21:31:40.195402Z","iopub.execute_input":"2023-02-24T21:31:40.196244Z","iopub.status.idle":"2023-02-24T21:31:45.909163Z","shell.execute_reply.started":"2023-02-24T21:31:40.196193Z","shell.execute_reply":"2023-02-24T21:31:45.907756Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"23/02/24 21:31:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"# Question 1:\nspark.version","metadata":{"execution":{"iopub.status.busy":"2023-02-24T21:31:45.911673Z","iopub.execute_input":"2023-02-24T21:31:45.913340Z","iopub.status.idle":"2023-02-24T21:31:45.926117Z","shell.execute_reply.started":"2023-02-24T21:31:45.913277Z","shell.execute_reply":"2023-02-24T21:31:45.924325Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'3.3.2'"},"metadata":{}}]},{"cell_type":"code","source":"! wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = spark.read.option(\"header\", \"true\").csv(\"/kaggle/working/fhvhv_tripdata_2021-06.csv.gz\")\ndf.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.rdd.getNumPartitions()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Question 2:\n# Repartition it to 12 partitions and save it to parquet.\ndf_12part = df.repartition(12)\ndf_12part.rdd.getNumPartitions()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_12part.write.parquet(\"df_12part.parquet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder = \"/kaggle/working/df_12part.parquet/\"\n\nfile_sizes = 0\nparquet_file_count = 0\nfor file in os.listdir(folder):\n    if file.endswith(\"parquet\"):\n        file_size = os.path.getsize(folder+file)\n        print(str(file_size).ljust(10), file)\n        file_sizes += file_size\n        parquet_file_count += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)?\nprint(\"avg parquet file size\", file_sizes/parquet_file_count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Question 3:\n# Count records\n# How many taxi trips were there on June 15?\n# Consider only trips that started on June 15.\ndf_12part = spark.read.parquet('/kaggle/working/df_12part.parquet')","metadata":{"execution":{"iopub.status.busy":"2023-02-24T21:31:58.380924Z","iopub.execute_input":"2023-02-24T21:31:58.381924Z","iopub.status.idle":"2023-02-24T21:32:04.011769Z","shell.execute_reply.started":"2023-02-24T21:31:58.381874Z","shell.execute_reply":"2023-02-24T21:32:04.010449Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"df_12part.rdd.first()","metadata":{"execution":{"iopub.status.busy":"2023-02-24T21:34:35.970259Z","iopub.execute_input":"2023-02-24T21:34:35.971466Z","iopub.status.idle":"2023-02-24T21:34:37.383053Z","shell.execute_reply.started":"2023-02-24T21:34:35.971410Z","shell.execute_reply":"2023-02-24T21:34:37.381803Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Row(dispatching_base_num='B02872', pickup_datetime='2021-06-12 21:48:21', dropoff_datetime='2021-06-12 22:01:16', PULocationID='125', DOLocationID='148', SR_Flag='N', Affiliated_base_number='B02872')"},"metadata":{}}]},{"cell_type":"code","source":"import pyspark.sql.functions as func\n\ndf = df_12part.select(func.to_date(df_12part.pickup_datetime).alias(\"time\"))\nsf = df.filter(df.time == \"2021-06-15\")\nsf.count()","metadata":{"execution":{"iopub.status.busy":"2023-02-24T21:53:45.211370Z","iopub.execute_input":"2023-02-24T21:53:45.211774Z","iopub.status.idle":"2023-02-24T21:53:47.271341Z","shell.execute_reply.started":"2023-02-24T21:53:45.211742Z","shell.execute_reply":"2023-02-24T21:53:47.270003Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"452470"},"metadata":{}}]},{"cell_type":"code","source":"# Question 4:\n# Longest trip for each day\n# Now calculate the duration for each trip.\n# How long was the longest trip in Hours?\n\n# func.datediff(func.to_date(df_12part.dropoff_datetime),func.to_date(df_12part.pickup_datetime))","metadata":{"execution":{"iopub.status.busy":"2023-02-24T22:08:19.232585Z","iopub.execute_input":"2023-02-24T22:08:19.233087Z","iopub.status.idle":"2023-02-24T22:08:19.237963Z","shell.execute_reply.started":"2023-02-24T22:08:19.233044Z","shell.execute_reply":"2023-02-24T22:08:19.236807Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Question 5:\n# User Interface\n# Sparkâ€™s User Interface which shows application's dashboard runs on which local port?\n4040","metadata":{"execution":{"iopub.status.busy":"2023-02-24T21:58:08.719622Z","iopub.execute_input":"2023-02-24T21:58:08.720860Z","iopub.status.idle":"2023-02-24T21:58:08.729109Z","shell.execute_reply.started":"2023-02-24T21:58:08.720793Z","shell.execute_reply":"2023-02-24T21:58:08.727783Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"4040"},"metadata":{}}]},{"cell_type":"code","source":"# Question 6:\n# Most frequent pickup location zone\n# Load the zone lookup data into a temp view in Spark\n# https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\n# Using the zone lookup data and the fhvhv June 2021 data, what is the name of the most frequent pickup location zone?\n# East Chelsea\n# Astoria\n# Union Sq\n# Crown Heights North","metadata":{"execution":{"iopub.status.busy":"2023-02-24T22:01:32.212252Z","iopub.execute_input":"2023-02-24T22:01:32.212722Z","iopub.status.idle":"2023-02-24T22:01:32.220488Z","shell.execute_reply.started":"2023-02-24T22:01:32.212685Z","shell.execute_reply":"2023-02-24T22:01:32.218767Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1440/3229186195.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    East Chelsea\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (3229186195.py, line 6)","output_type":"error"}]}]}